{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from utils_nlp.azureml.azureml_utils import get_or_create_workspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ws = get_or_create_workspace(\n",
    "    subscription_id=\"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\",\n",
    "    resource_group=\"nlprg\",\n",
    "    workspace_name=\"MAIDAIPBERT-eastus\",\n",
    "    workspace_region=\"East US\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    ws = get_or_create_workspace(\n",
    "    subscription_id=\"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\",\n",
    "    resource_group=\"nlprg\",\n",
    "    workspace_name=\"MAIDAIPBERT-eastus\",\n",
    "    workspace_region=\"East US\",\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: MAIDAIPBERT-eastus\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "print(\"Workspace name: {}\".format(ws.name))\n",
    "print(\"Resource group: {}\".format(ws.resource_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"bertncrs24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found compute target: bertncrs24\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 1, 'preemptedNodeCount': 0}, 'allocationState': 'Resizing', 'allocationStateTransitionTime': '2019-07-24T20:58:25.663000+00:00', 'errors': None, 'creationTime': '2019-07-12T19:59:45.933132+00:00', 'modifiedTime': '2019-07-12T20:00:01.793458+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC24RS_V3'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found compute target: {}\".format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(cluster_name))\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=1\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./entail\\\\utils_nlp'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = True\n",
    "project_dir = \"./entail\"\n",
    "if DEBUG and os.path.exists(project_dir): \n",
    "    shutil.rmtree(project_dir) \n",
    "shutil.copytree(\"../../utils_nlp\", os.path.join(project_dir, \"utils_nlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./entail/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_dir/train.py\n",
    "\n",
    "import horovod.torch as hvd\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from utils_nlp.dataset.xnli_dataset import XnliDataset\n",
    "from utils_nlp.bert.common import Language\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from utils_nlp.bert.sequence_classification_distributed import BERTSequenceClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from azureml.core.run import Run\n",
    "# get the Azure ML run object\n",
    "run = Run.get_context()\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "LANGUAGE_ENGLISH = \"en\"\n",
    "CACHE_DIR = \"./\"\n",
    "TRAIN_FILE_SPLIT = \"train\"\n",
    "TEST_FILE_SPLIT = \"test\"\n",
    "TO_LOWERCASE = True \n",
    "PRETRAINED_BERT_LNG = Language.ENGLISH\n",
    "\n",
    "# optimizer configurations\n",
    "LEARNING_RATE= 5e-5\n",
    "WARMUP_PROPORTION= 0.1\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "NUM_GPUS = 4\n",
    "\n",
    "hvd.init()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Training settings\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',help='random seed (default: 42)')\n",
    "parser.add_argument('--epochs', type=int, default=2, metavar='N', help='number of epochs to train (default: 2)')\n",
    "parser.add_argument('--num_workers', type=int, default=2, metavar='N', help='number of workers to train (default: 2)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,help='disables CUDA training')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(args.cuda)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "#kwargs = {}\n",
    "kwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_dataset = XnliDataset(file_split=TRAIN_FILE_SPLIT, \n",
    "                            cache_dir=CACHE_DIR, \n",
    "                            language=LANGUAGE_ENGLISH,\n",
    "                            to_lowercase=TO_LOWERCASE,\n",
    "                            tok_language=PRETRAINED_BERT_LNG)\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.local_rank())\n",
    "train_loader =  DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, **kwargs)\n",
    "    \n",
    "#set the label_encoder for evaluation datset\n",
    "label_encoder = train_dataset.label_encoder\n",
    "num_labels = len(np.unique(train_dataset.labels))\n",
    "\n",
    "print(\"============================= Data set size ========================\")\n",
    "classifier = BERTSequenceClassifier(language=PRETRAINED_BERT_LNG,\n",
    "                                            num_labels=num_labels,\n",
    "                                            cache_dir=CACHE_DIR,\n",
    "                                            )\n",
    "\n",
    "# optimizer configurations\n",
    "num_samples = len(train_loader.dataset)\n",
    "num_batches = int(num_samples/BATCH_SIZE)\n",
    "num_workers = args.num_workers\n",
    "num_train_optimization_steps = num_batches*args.epochs #int(num_batches/hvd.size()) * args.epochs \n",
    "optimizer_grouped_parameters = classifier.optimizer_params\n",
    "\n",
    "print(\"================= num_train_optimization_steps ==============================\")\n",
    "print(num_train_optimization_steps)\n",
    "\n",
    "lr=LEARNING_RATE * hvd.size()\n",
    "\n",
    "bert_optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                   lr=lr,\n",
    "                   t_total=num_train_optimization_steps,\n",
    "                   warmup=WARMUP_PROPORTION,)\n",
    "\n",
    "if WARMUP_PROPORTION is None:\n",
    "    print(\"================== Without Warmup proprtion ===========================\")\n",
    "    bert_optimizer = BertAdam(optimizer_grouped_parameters, lr=lr)\n",
    "else:\n",
    "    print(\"================== With Warmup proportion =============================\")\n",
    "    bert_optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                   lr=lr,\n",
    "                   t_total=num_train_optimization_steps,\n",
    "                   warmup=WARMUP_PROPORTION,\n",
    "                  )\n",
    "\n",
    "\n",
    "## Distributed optimizer\n",
    "bert_optimizer = hvd.DistributedOptimizer(bert_optimizer, classifier.model.named_parameters())\n",
    "hvd.broadcast_parameters(classifier.model.state_dict(), root_rank=0)\n",
    "\n",
    "#remove later\n",
    "if(hvd.rank() == 0):\n",
    "    print(\"===================== rank rank =======================\", hvd.rank())\n",
    "else:\n",
    "    print(\"===== not master rank =================================\")\n",
    "    \n",
    "\n",
    "classifier.fit(train_loader, bert_optimizer, args.epochs, NUM_GPUS, hvd.rank())\n",
    "\n",
    "#evaluation\n",
    "if(hvd.rank() == 0):\n",
    "    kwargs = {}\n",
    "    test_dataset = XnliDataset(file_split=TEST_FILE_SPLIT,\n",
    "                           cache_dir=CACHE_DIR,\n",
    "                           language=LANGUAGE_ENGLISH,\n",
    "                           to_lowercase=TO_LOWERCASE,\n",
    "                           tok_language=PRETRAINED_BERT_LNG\n",
    "                          )    \n",
    "    test_sampler = SequentialSampler(test_dataset)  \n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler, **kwargs)\n",
    "    \n",
    "    predictions = classifier.predict(test_loader, NUM_GPUS, BATCH_SIZE, probabilities=False)\n",
    "    \n",
    "    print('=================== Predictions =====================')\n",
    "    print(predictions)\n",
    "\n",
    "    test_dict = next(iter(test_loader))\n",
    "    test_labels = test_dict['labels']\n",
    "    predictions= label_encoder.inverse_transform(predictions)\n",
    "    print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import azureml.core\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_COUNT = 2\n",
    "mpiConfig=MpiConfiguration()\n",
    "mpiConfig.process_count_per_node=4\n",
    "\n",
    "est = PyTorch(\n",
    "    source_directory=project_dir,\n",
    "    compute_target=compute_target,\n",
    "    entry_script=\"train.py\",\n",
    "    node_count=NODE_COUNT,\n",
    "    distributed_training=mpiConfig,\n",
    "    use_gpu=True,\n",
    "    framework_version=\"1.0\",\n",
    "    conda_packages=[\"scikit-learn=0.20.3\", \"numpy\", \"spacy\", \"nltk\"],\n",
    "    pip_packages=[\"pandas\",\"seqeval[gpu]\", \"pytorch-pretrained-bert\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Azure ML SDK Version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "experiment = Experiment(ws, name=\"nlp-entailment-bert\")\n",
    "run = experiment.submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cfb77737784d6484d719a4bc203dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='outputs', model_path='outputs')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This statement downloads the model to local and you can use this to run predictions locally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.download(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is this requied to call files from the datastore for deployment ??\n",
    "DEBUG = True\n",
    "project_dir = \"./entailment_aml\"\n",
    "if DEBUG and os.path.exists(project_dir): \n",
    "    shutil.rmtree(project_dir) \n",
    "shutil.copytree(\"../../utils_nlp\", os.path.join(project_dir, \"utils_nlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_nlp.dataset.xnli import load_pandas_df\n",
    "test_df = load_pandas_df(local_cache_path=\"../../temp\", file_split=\"test\", language=\"en\")\n",
    "\n",
    "test_data_used_count = round(0.0025 * test_df.shape[0])\n",
    "test_df = test_df.loc[:test_data_used_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load('bert_entailment.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import torch\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import SequentialSampler, DataLoader\n",
    "from utils_nlp.dataset.xnli_dataset_test import XnliDatasetTest\n",
    "from utils_nlp.bert.common import Language\n",
    "from utils_nlp.bert.sequence_classification_distributed import BERTSequenceClassifier\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    \n",
    "    model_path = Model.get_model_path('bert_entailment')\n",
    "    print('Model Path ::', model_path)\n",
    "    #model = model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "def run():\n",
    "\n",
    "    LANGUAGE_ENGLISH = Language.ENGLISH\n",
    "    CACHE_DIR = \"../../temp\"\n",
    "    \n",
    "    print(\"===============load model=============\")\n",
    "    \n",
    "    model_path = 'outputs/bert-large-uncased'\n",
    "    model = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "    bert_model = BERTSequenceClassifier(language=LANGUAGE_ENGLISH,\n",
    "                  num_labels=3,\n",
    "                  cache_dir=CACHE_DIR,\n",
    "                 )\n",
    "    \n",
    "    #bert_model.model = model\n",
    "    #print(\"============= bert model ==================\")\n",
    "    #print(bert_model.model)\n",
    "    \n",
    "    num_gpus = 0\n",
    "    batch_size = 32\n",
    "    probabilities = False\n",
    "    test = XnliDatasetTest()\n",
    "    test_dataset = test\n",
    "    test_sampler = SequentialSampler(test_dataset)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"make predictions now please :(\")\n",
    "    predictions = bert_model.predict(test_loader, num_gpus, batch_size, probabilities)\n",
    "    print('=================== Predictions =====================')\n",
    "    print(predictions)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import score\n",
    "predictions = score.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
