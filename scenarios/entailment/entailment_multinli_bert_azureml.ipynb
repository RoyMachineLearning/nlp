{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference on MultiNLI Dataset using BERT with Azure Machine Learning\n",
    "\n",
    "## Summary\n",
    "In this notebook, we demostrate using the BERT model to do language inference in English. We use the [XNLI](https://github.com/facebookresearch/XNLI) dataset and the task is to classify sentence pairs into three classes: contradiction, entailment, and neutral.   \n",
    "The figure below shows how [BERT](https://arxiv.org/abs/1810.04805) classifies sentence pairs. It concatenates the tokens in each sentence pairs and separates the sentences by the [SEP] token. A [CLS] token is prepended to the token list and used as the aggregate sequence representation for the classification task.\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_two_sentence.PNG\">\n",
    "\n",
    "Azure Machine Learning features higlighted in the notebook : \n",
    "\n",
    "- Distributed training with Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import azureml.core\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from utils_nlp.azureml.azureml_utils import get_or_create_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AzureML Setup\n",
    "\n",
    "### 2.1 Link to or create a Workspace\n",
    "\n",
    "First, go through the [Configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`. This will create a config.json file containing the values needed below to create a workspace.\n",
    "\n",
    "**Note**: you do not need to fill in these values if you have a config.json in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ws = get_or_create_workspace(\n",
    "    subscription_id=\"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\",\n",
    "    resource_group=\"nlprg\",\n",
    "    workspace_name=\"MAIDAIPBERT-eastus\",\n",
    "    workspace_region=\"East US\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Workspace name: \" + ws.name,\n",
    "    \"Azure region: \" + ws.location,\n",
    "    \"Subscription id: \" + ws.subscription_id,\n",
    "    \"Resource group: \" + ws.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Link AmlCompute Compute Target\n",
    "\n",
    "We need to link a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training our model (see [compute options](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#supported-compute-targets) for explanation of the different options). We will use an [AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) target and link to an existing target (if the cluster_name exists) or create a STANDARD_NC6 GPU cluster (autoscales from 0 to 4 nodes) in this example. Creating a new AmlComputes takes approximately 5 minutes. \n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_name = \"bertncrs24\"\n",
    "#cluster_name = \"gpu-entail\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found compute target: {}\".format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(cluster_name))\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=1\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "project_dir = \"./entail_utils\"\n",
    "if DEBUG and os.path.exists(project_dir): \n",
    "    shutil.rmtree(project_dir) \n",
    "shutil.copytree(\"../../utils_nlp\", os.path.join(project_dir, \"utils_nlp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing $project_dir/train.py\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '$project_dir/train.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a58eb6b87dd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'writefile'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'$project_dir/train.py'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\nimport horovod.torch as hvd\\nimport torch\\nimport numpy as np\\nimport time\\nimport argparse\\nfrom torch.utils.data import DataLoader\\nfrom utils_nlp.dataset.xnli_torch_dataset import XnliDataset\\nfrom utils_nlp.models.bert.common import Language\\nfrom pytorch_pretrained_bert.optimization import BertAdam\\nfrom utils_nlp.models.bert.sequence_classification import BERTSequenceClassifier\\nfrom sklearn.metrics import classification_report\\n\\nprint(\"Torch version:\", torch.__version__)\\n\\nhvd.init()\\n\\nLANGUAGE_ENGLISH = \"en\"\\nTRAIN_FILE_SPLIT = \"train\"\\nTEST_FILE_SPLIT = \"test\"\\nTO_LOWERCASE = True \\nPRETRAINED_BERT_LNG = Language.ENGLISH\\nLEARNING_RATE= 5e-5\\nWARMUP_PROPORTION= 0.1\\nBATCH_SIZE = 32\\nNUM_GPUS = 4\\n\\n## each machine gets it\\'s own copy of data\\nCACHE_DIR = \\'./xnli-data-%d\\' % hvd.rank()\\n\\nparser = argparse.ArgumentParser()\\n# Training settings\\nparser.add_argument(\\'--seed\\', type=int, default=42, metavar=\\'S\\',help=\\'random seed (default: 42)\\')\\nparser.add_argument(\\'--no-cuda\\', action=\\'store_true\\', default=False,help=\\'disables CUDA training\\')\\n\\nargs = parser.parse_args()\\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\\n\\n\\n\\'\\'\\'\\nNote: For example, you have 4 nodes and 4 GPUs each node, so you spawn 16 workers. \\nEvery worker will have a rank [0, 15], and every worker will have a local_rank [0, 3]\\n\\'\\'\\'\\nif args.cuda:\\n    torch.cuda.set_device(hvd.local_rank())\\n    torch.cuda.manual_seed(args.seed)\\n\\n#num_workers - this is equal to number of gpus per machine \\nkwargs = {\\'num_workers\\': 4, \\'pin_memory\\': True} if args.cuda else {}\\n\\ntrain_dataset = XnliDataset(file_split=TRAIN_FILE_SPLIT, \\n                            cache_dir=CACHE_DIR, \\n                            language=LANGUAGE_ENGLISH,\\n                            to_lowercase=TO_LOWERCASE,\\n                            tok_language=PRETRAINED_BERT_LNG)\\n\\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\\ntrain_loader =  DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, **kwargs)\\n    \\n#set the label_encoder for evaluation\\nlabel_encoder = train_dataset.label_encoder\\nnum_labels = len(np.unique(train_dataset.labels))\\n\\nclassifier = BERTSequenceClassifier(language=PRETRAINED_BERT_LNG,\\n                                            num_labels=num_labels,\\n                                            cache_dir=CACHE_DIR,\\n                                            )\\n\\n# optimizer configurations\\nnum_samples = len(train_loader.dataset)\\nnum_batches = int(num_samples/BATCH_SIZE)\\nnum_workers = args.num_workers\\nnum_train_optimization_steps = num_batches*args.epochs #int(num_batches/hvd.size()) * args.epochs \\noptimizer_grouped_parameters = classifier.optimizer_params\\n\\nlr=LEARNING_RATE * hvd.size()\\n\\nbert_optimizer = BertAdam(optimizer_grouped_parameters,\\n                   lr=lr,\\n                   t_total=num_train_optimization_steps,\\n                   warmup=WARMUP_PROPORTION,)\\n\\nif WARMUP_PROPORTION is None:\\n    bert_optimizer = BertAdam(optimizer_grouped_parameters, lr=lr)\\nelse:\\n    bert_optimizer = BertAdam(optimizer_grouped_parameters,\\n                   lr=lr,\\n                   t_total=num_train_optimization_steps,\\n                   warmup=WARMUP_PROPORTION,\\n                  )\\n\\n\\n## Distributed optimizer\\nbert_optimizer = hvd.DistributedOptimizer(bert_optimizer, classifier.model.named_parameters())\\nhvd.broadcast_parameters(classifier.model.state_dict(), root_rank=0)\\n    \\nclassifier.fit(train_loader, bert_optimizer, args.epochs, NUM_GPUS, hvd.rank())\\n\\n#evaluation\\nif(hvd.rank() == 0):\\n    NUM_GPUS = 0\\n    kwargs = {}\\n    test_dataset = XnliDataset(file_split=TEST_FILE_SPLIT,\\n                           cache_dir=CACHE_DIR,\\n                           language=LANGUAGE_ENGLISH,\\n                           to_lowercase=TO_LOWERCASE,\\n                           tok_language=PRETRAINED_BERT_LNG\\n                          )    \\n    \\n    test_loader = DataLoader(test_dataset, **kwargs)\\n    \\n    predictions = classifier.predict(test_loader, NUM_GPUS, probabilities=False)\\n    print(\\'=================== Predictions =====================\\')\\n    print(predictions)\\n\\n    test_labels = []\\n    for data in test_dataset:\\n        test_labels.append(data[\\'labels\\'])\\n        \\n    predictions= label_encoder.inverse_transform(predictions)\\n    print(classification_report(test_labels, predictions))\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\jamahaja\\appdata\\local\\continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2350\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2352\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<c:\\users\\jamahaja\\appdata\\local\\continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\decorator.py:decorator-gen-106>\u001b[0m in \u001b[0;36mwritefile\u001b[1;34m(self, line, cell)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jamahaja\\appdata\\local\\continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jamahaja\\appdata\\local\\continuum\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\magics\\osm.py\u001b[0m in \u001b[0;36mwritefile\u001b[1;34m(self, line, cell)\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'a'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 842\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    843\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '$project_dir/train.py'"
     ]
    }
   ],
   "source": [
    "%%writefile $project_dir/train.py\n",
    "\n",
    "import horovod.torch as hvd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from utils_nlp.dataset.xnli_torch_dataset import XnliDataset\n",
    "from utils_nlp.models.bert.common import Language\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from utils_nlp.models.bert.sequence_classification import BERTSequenceClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "hvd.init()\n",
    "\n",
    "LANGUAGE_ENGLISH = \"en\"\n",
    "TRAIN_FILE_SPLIT = \"train\"\n",
    "TEST_FILE_SPLIT = \"test\"\n",
    "TO_LOWERCASE = True \n",
    "PRETRAINED_BERT_LNG = Language.ENGLISH\n",
    "LEARNING_RATE= 5e-5\n",
    "WARMUP_PROPORTION= 0.1\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 4\n",
    "\n",
    "## each machine gets it's own copy of data\n",
    "CACHE_DIR = './xnli-data-%d' % hvd.rank()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Training settings\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',help='random seed (default: 42)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,help='disables CUDA training')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "\n",
    "'''\n",
    "Note: For example, you have 4 nodes and 4 GPUs each node, so you spawn 16 workers. \n",
    "Every worker will have a rank [0, 15], and every worker will have a local_rank [0, 3]\n",
    "'''\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "#num_workers - this is equal to number of gpus per machine \n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_dataset = XnliDataset(file_split=TRAIN_FILE_SPLIT, \n",
    "                            cache_dir=CACHE_DIR, \n",
    "                            language=LANGUAGE_ENGLISH,\n",
    "                            to_lowercase=TO_LOWERCASE,\n",
    "                            tok_language=PRETRAINED_BERT_LNG)\n",
    "\n",
    "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "train_loader =  DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, **kwargs)\n",
    "    \n",
    "#set the label_encoder for evaluation\n",
    "label_encoder = train_dataset.label_encoder\n",
    "num_labels = len(np.unique(train_dataset.labels))\n",
    "\n",
    "classifier = BERTSequenceClassifier(language=PRETRAINED_BERT_LNG,\n",
    "                                            num_labels=num_labels,\n",
    "                                            cache_dir=CACHE_DIR,\n",
    "                                            )\n",
    "\n",
    "# optimizer configurations\n",
    "num_samples = len(train_loader.dataset)\n",
    "num_batches = int(num_samples/BATCH_SIZE)\n",
    "num_workers = args.num_workers\n",
    "num_train_optimization_steps = num_batches*args.epochs #int(num_batches/hvd.size()) * args.epochs \n",
    "optimizer_grouped_parameters = classifier.optimizer_params\n",
    "\n",
    "lr=LEARNING_RATE * hvd.size()\n",
    "\n",
    "bert_optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                   lr=lr,\n",
    "                   t_total=num_train_optimization_steps,\n",
    "                   warmup=WARMUP_PROPORTION,)\n",
    "\n",
    "if WARMUP_PROPORTION is None:\n",
    "    bert_optimizer = BertAdam(optimizer_grouped_parameters, lr=lr)\n",
    "else:\n",
    "    bert_optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                   lr=lr,\n",
    "                   t_total=num_train_optimization_steps,\n",
    "                   warmup=WARMUP_PROPORTION,\n",
    "                  )\n",
    "\n",
    "\n",
    "## Distributed optimizer\n",
    "bert_optimizer = hvd.DistributedOptimizer(bert_optimizer, classifier.model.named_parameters())\n",
    "hvd.broadcast_parameters(classifier.model.state_dict(), root_rank=0)\n",
    "    \n",
    "classifier.fit(train_loader, bert_optimizer, args.epochs, NUM_GPUS, hvd.rank())\n",
    "\n",
    "#evaluation\n",
    "if(hvd.rank() == 0):\n",
    "    NUM_GPUS = 0\n",
    "    kwargs = {}\n",
    "    test_dataset = XnliDataset(file_split=TEST_FILE_SPLIT,\n",
    "                           cache_dir=CACHE_DIR,\n",
    "                           language=LANGUAGE_ENGLISH,\n",
    "                           to_lowercase=TO_LOWERCASE,\n",
    "                           tok_language=PRETRAINED_BERT_LNG\n",
    "                          )    \n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, **kwargs)\n",
    "    \n",
    "    predictions = classifier.predict(test_loader, NUM_GPUS, probabilities=False)\n",
    "    print('=================== Predictions =====================')\n",
    "    print(predictions)\n",
    "\n",
    "    test_labels = []\n",
    "    for data in test_dataset:\n",
    "        test_labels.append(data['labels'])\n",
    "        \n",
    "    predictions= label_encoder.inverse_transform(predictions)\n",
    "    print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a PyTorch Estimator\n",
    "\n",
    "BERT is built on PyTorch, so we will use the AzureML SDK's PyTorch estimator to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, see [How to Train Pytorch Models on AzureML](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch). First we set up a .yml file with the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_COUNT = 2\n",
    "mpiConfig=MpiConfiguration()\n",
    "mpiConfig.process_count_per_node=4\n",
    "\n",
    "est = PyTorch(\n",
    "    source_directory=project_dir,\n",
    "    compute_target=compute_target,\n",
    "    entry_script=\"train.py\",\n",
    "    node_count=NODE_COUNT,\n",
    "    distributed_training=mpiConfig,\n",
    "    use_gpu=True,\n",
    "    framework_version=\"1.0\",\n",
    "    conda_packages=[\"scikit-learn=0.20.3\", \"numpy\", \"spacy\", \"nltk\"],\n",
    "    pip_packages=[\"pandas\",\"seqeval[gpu]\", \"pytorch-pretrained-bert\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Experiment and Submit a Job\n",
    "Submit the estimator object to run your experiment. Results can be monitored using a Jupyter widget. The widget and run are asynchronous and update every 10-15 seconds until job completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, name=\"NLP-Entailment-BERT\")\n",
    "run = experiment.submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
