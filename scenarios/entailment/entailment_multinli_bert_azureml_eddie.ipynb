{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference on MultiNLI Dataset using BERT with Azure Machine Learning\n",
    "\n",
    "## Summary\n",
    "In this notebook, we demostrate using the BERT model to do language inference in English. We use the [XNLI](https://github.com/facebookresearch/XNLI) dataset and the task is to classify sentence pairs into three classes: contradiction, entailment, and neutral.   \n",
    "The figure below shows how [BERT](https://arxiv.org/abs/1810.04805) classifies sentence pairs. It concatenates the tokens in each sentence pairs and separates the sentences by the [SEP] token. A [CLS] token is prepended to the token list and used as the aggregate sequence representation for the classification task.\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/bert_two_sentence.PNG\">\n",
    "\n",
    "Azure Machine Learning features higlighted in the notebook : \n",
    "\n",
    "- Distributed training with Horovod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import azureml.core\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from utils_nlp.azureml.azureml_utils import get_or_create_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AzureML Setup\n",
    "\n",
    "### 2.1 Link to or create a Workspace\n",
    "\n",
    "First, go through the [Configuration](https://github.com/Azure/MachineLearningNotebooks/blob/master/configuration.ipynb) notebook to install the Azure Machine Learning Python SDK and create an Azure ML `Workspace`. This will create a config.json file containing the values needed below to create a workspace.\n",
    "\n",
    "**Note**: you do not need to fill in these values if you have a config.json in the same folder as this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ws = get_or_create_workspace(\n",
    "    subscription_id=\"15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\",\n",
    "    resource_group=\"nlprg\",\n",
    "    workspace_name=\"MAIDAIPBERT-eastus\",\n",
    "    workspace_region=\"East US\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: MAIDAIPBERT-eastus\n",
      "Azure region: eastus\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: nlprg\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Workspace name: \" + ws.name,\n",
    "    \"Azure region: \" + ws.location,\n",
    "    \"Subscription id: \" + ws.subscription_id,\n",
    "    \"Resource group: \" + ws.resource_group,\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Link AmlCompute Compute Target\n",
    "\n",
    "We need to link a [compute target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) for training our model (see [compute options](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#supported-compute-targets) for explanation of the different options). We will use an [AmlCompute](https://docs.microsoft.com/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute) target and link to an existing target (if the cluster_name exists) or create a STANDARD_NC6 GPU cluster (autoscales from 0 to 4 nodes) in this example. Creating a new AmlComputes takes approximately 5 minutes. \n",
    "\n",
    "As with other Azure services, there are limits on certain resources (e.g. AmlCompute) associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-quotas) on the default limits and how to request more quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found compute target: bertncrs24\n",
      "{'currentNodeCount': 2, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 2, 'preemptedNodeCount': 0}, 'allocationState': 'Resizing', 'allocationStateTransitionTime': '2019-08-01T04:39:40.064000+00:00', 'errors': None, 'creationTime': '2019-07-12T19:59:45.933132+00:00', 'modifiedTime': '2019-07-12T20:00:01.793458+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC24RS_V3'}\n"
     ]
    }
   ],
   "source": [
    "cluster_name = \"bertncrs24\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print(\"Found compute target: {}\".format(cluster_name))\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target: {}\".format(cluster_name))\n",
    "    compute_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_NC6\", max_nodes=1\n",
    "    )\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "\n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./entail_utils\\\\utils_nlp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEBUG = True\n",
    "project_dir = \"./entail_utils\"\n",
    "if DEBUG and os.path.exists(project_dir): \n",
    "    shutil.rmtree(project_dir) \n",
    "shutil.copytree(\"../../utils_nlp\", os.path.join(project_dir, \"utils_nlp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./entail_utils/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $project_dir/train.py\n",
    "\n",
    "import horovod.torch as hvd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "#from torch.utils.data import DataLoader\n",
    "from utils_nlp.dataset.xnli_torch_dataset import XnliDataset\n",
    "from utils_nlp.models.bert.common import Language\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from utils_nlp.models.bert.sequence_classification import BERTSequenceClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "#hvd.init()\n",
    "\n",
    "LANGUAGE_ENGLISH = \"en\"\n",
    "TRAIN_FILE_SPLIT = \"train\"\n",
    "TEST_FILE_SPLIT = \"test\"\n",
    "TO_LOWERCASE = True \n",
    "PRETRAINED_BERT_LNG = Language.ENGLISH\n",
    "LEARNING_RATE= 5e-5\n",
    "WARMUP_PROPORTION= 0.1\n",
    "BATCH_SIZE = 32\n",
    "NUM_GPUS = 4\n",
    "\n",
    "## each machine gets it's own copy of data\n",
    "CACHE_DIR = './xnli-data-%d' % hvd.rank()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Training settings\n",
    "parser.add_argument('--seed', type=int, default=42, metavar='S',help='random seed (default: 42)')\n",
    "parser.add_argument('--epochs', type=int, default=2, metavar='S',help='random seed (default: 2)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,help='disables CUDA training')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "\n",
    "'''\n",
    "Note: For example, you have 4 nodes and 4 GPUs each node, so you spawn 16 workers. \n",
    "Every worker will have a rank [0, 15], and every worker will have a local_rank [0, 3]\n",
    "'''\n",
    "if args.cuda:\n",
    "    torch.cuda.set_device(hvd.local_rank())\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "#num_workers - this is equal to number of gpus per machine \n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if args.cuda else {}\n",
    "\n",
    "train_dataset = XnliDataset(file_split=TRAIN_FILE_SPLIT, \n",
    "                            cache_dir=CACHE_DIR, \n",
    "                            language=LANGUAGE_ENGLISH,\n",
    "                            to_lowercase=TO_LOWERCASE,\n",
    "                            tok_language=PRETRAINED_BERT_LNG)\n",
    "\n",
    "#train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\n",
    "# train_loader =  DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, **kwargs)\n",
    "train_loader = BERTSequenceClassifier.create_loader(train_dataset)\n",
    "#set the label_encoder for evaluation\n",
    "label_encoder = train_dataset.label_encoder\n",
    "num_labels = len(np.unique(train_dataset.labels))\n",
    "\n",
    "classifier = BERTSequenceClassifier(language=PRETRAINED_BERT_LNG, num_labels=num_labels, cache_dir=CACHE_DIR)\n",
    "# create optimizer start is called by default internally\n",
    "# optimizer configurations\n",
    "# num_samples = len(train_loader.dataset)\n",
    "# num_batches = int(num_samples / BATCH_SIZE)\n",
    "# num_train_optimization_steps = num_batches*args.epochs \n",
    "# optimizer_grouped_parameters = classifier.optimizer_params\n",
    "\n",
    "# lr=LEARNING_RATE * hvd.size()\n",
    "\n",
    "# bert_optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "#                    lr=lr,\n",
    "#                    t_total=num_train_optimization_steps,\n",
    "#                    warmup=WARMUP_PROPORTION,)\n",
    "\n",
    "# if WARMUP_PROPORTION is None:\n",
    "#     bert_optimizer = BertAdam(optimizer_grouped_parameters, lr=lr)\n",
    "# else:\n",
    "#     bert_optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "#                    lr=lr,\n",
    "#                    t_total=num_train_optimization_steps,\n",
    "#                    warmup=WARMUP_PROPORTION,\n",
    "#                   )\n",
    "# create optimizer end\n",
    "\n",
    "## Distributed optimizer\n",
    "#bert_optimizer = hvd.DistributedOptimizer(bert_optimizer, classifier.model.named_parameters())\n",
    "#hvd.broadcast_parameters(classifier.model.state_dict(), root_rank=0)\n",
    "\n",
    "train_loader = classifier.create_train_loader()\n",
    "# bert_optimizer = classifier.create_optimizer() # called internally if optimizer=None\n",
    "\n",
    "classifier.fit(train_loader, args.epochs, NUM_GPUS)\n",
    "for epoch in range(args.epochs):\n",
    "    classifier.train(train_loader, optimizer=bert_optimizer, num_gpus=NUM_GPUS)\n",
    "\n",
    "#evaluation\n",
    "if(hvd.rank() == 0):\n",
    "    classifier.model.save_model()  # add it here\n",
    "    NUM_GPUS = 1\n",
    "    kwargs = {}\n",
    "    test_dataset = XnliDataset(file_split=TEST_FILE_SPLIT,\n",
    "                           cache_dir=CACHE_DIR,\n",
    "                           language=LANGUAGE_ENGLISH,\n",
    "                           to_lowercase=TO_LOWERCASE,\n",
    "                           tok_language=PRETRAINED_BERT_LNG\n",
    "                          )    \n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, **kwargs)\n",
    "    \n",
    "    predictions = classifier.predict(test_loader, NUM_GPUS, BATCH_SIZE, probabilities=False)\n",
    "    print('=================== Predictions =====================')\n",
    "    print(predictions)\n",
    "\n",
    "    test_labels = []\n",
    "    for data in test_dataset:\n",
    "        test_labels.append(data['labels'])\n",
    "        \n",
    "    predictions= label_encoder.inverse_transform(predictions)\n",
    "    print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a PyTorch Estimator\n",
    "\n",
    "BERT is built on PyTorch, so we will use the AzureML SDK's PyTorch estimator to easily submit PyTorch training jobs for both single-node and distributed runs. For more information on the PyTorch estimator, see [How to Train Pytorch Models on AzureML](https://docs.microsoft.com/azure/machine-learning/service/how-to-train-pytorch). First we set up a .yml file with the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NODE_COUNT = 2\n",
    "mpiConfig=MpiConfiguration()\n",
    "mpiConfig.process_count_per_node=4\n",
    "\n",
    "est = PyTorch(\n",
    "    source_directory=project_dir,\n",
    "    compute_target=compute_target,\n",
    "    entry_script=\"train.py\",\n",
    "    node_count=NODE_COUNT,\n",
    "    distributed_training=mpiConfig,\n",
    "    use_gpu=True,\n",
    "    framework_version=\"1.0\",\n",
    "    conda_packages=[\"scikit-learn=0.20.3\", \"numpy\", \"spacy\", \"nltk\"],\n",
    "    pip_packages=[\"pandas\",\"seqeval[gpu]\", \"pytorch-pretrained-bert\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Experiment and Submit a Job\n",
    "Submit the estimator object to run your experiment. Results can be monitored using a Jupyter widget. The widget and run are asynchronous and update every 10-15 seconds until job completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, name=\"NLP-Entailment-BERT\")\n",
    "run = experiment.submit(est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2255a5f9dd6043e69b5f6f8d34459f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Retrying (Retry(total=2, connect=2, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x00000244EFC6E7F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',)': /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/nlprg/providers/Microsoft.MachineLearningServices/workspaces/MAIDAIPBERT-eastus/experiments/NLP-Entailment-BERT/runs/NLP-Entailment-BERT_1564634498_8d59c107\n",
      "WARNING - Retrying (Retry(total=1, connect=1, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x00000244EFC6E4A8>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',)': /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/nlprg/providers/Microsoft.MachineLearningServices/workspaces/MAIDAIPBERT-eastus/experiments/NLP-Entailment-BERT/runs/NLP-Entailment-BERT_1564634498_8d59c107\n"
     ]
    }
   ],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
